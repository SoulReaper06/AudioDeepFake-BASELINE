{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply augmentation only on Positive Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for preprocessing data with audio augmentation\n",
    "def preprocess_data_with_augmentation(file_path, max_time_steps=109, sample_rate=16000, duration=3, n_mels=80):\n",
    "    audio, _ = librosa.load(file_path, sr=sample_rate, duration=duration)\n",
    "\n",
    "    # Apply data augmentation\n",
    "    pitch_shifted_audio = librosa.effects.pitch_shift(audio,sr=sample_rate, n_steps=np.random.uniform(-2, 2))\n",
    "    time_stretched_audio = librosa.effects.time_stretch(audio, rate=np.random.uniform(0.8, 1.2))\n",
    "    amplitude_scaled_audio = audio * np.random.uniform(0.5, 1.5)\n",
    "\n",
    "    augmented_audios = [audio, pitch_shifted_audio, time_stretched_audio, amplitude_scaled_audio]\n",
    "\n",
    "    mel_spectrograms = []\n",
    "\n",
    "    for augmented_audio in augmented_audios:\n",
    "        # Extract Mel spectrogram using librosa\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=augmented_audio, sr=sample_rate, n_mels=n_mels)\n",
    "        mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "        # Ensure all spectrograms have the same width (time steps)\n",
    "        if mel_spectrogram.shape[1] < max_time_steps:\n",
    "            mel_spectrogram = np.pad(mel_spectrogram, ((0, 0), (0, max_time_steps - mel_spectrogram.shape[1])), mode='constant')\n",
    "        else:\n",
    "            mel_spectrogram = mel_spectrogram[:, :max_time_steps]\n",
    "\n",
    "        mel_spectrograms.append(mel_spectrogram)\n",
    "\n",
    "    return mel_spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your file paths and constants\n",
    "TRAINING_LABEL = '/data/common_source/datasets/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt'\n",
    "TRAINING_DATA = '/data/common_source/datasets/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_train/flac'\n",
    "VALIDATION_DATA = '/data/common_source/datasets/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_dev/flac'\n",
    "VALIDATION_LABEL = '/data/common_source/datasets/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.dev.trl.txt'\n",
    "SAMPLE_RATE = 16000  # Adjust if your sample rate is different\n",
    "DURATION = 3  # Adjust the duration of your audio samples\n",
    "N_MELS = 80  # Adjust the number of mel filters\n",
    "max_time_steps = 109 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels for training Data\n",
    "if os.path.exists('labels/X_train_MEL.npy') and os.path.exists('labels/y_train_MEL.npy'):\n",
    "    X = np.load('labels/X_train_MEL.npy')\n",
    "    y = np.load('labels/y_train_MEL.npy')\n",
    "else:\n",
    "    train_labels = {}\n",
    "\n",
    "    with open(TRAINING_LABEL, 'r') as label_file:\n",
    "        lines = label_file.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        file_name = parts[1]\n",
    "        label = 1 if parts[-1] == \"bonafide\" else 0\n",
    "        train_labels[file_name] = label\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    " # Define the maximum time steps for your model\n",
    "\n",
    "    for file_name, label in train_labels.items():\n",
    "        file_path = os.path.join(TRAINING_DATA, file_name + \".flac\")\n",
    "\n",
    "        # Use the preprocess_data function\n",
    "        mel_spectrogram = preprocess_data_with_augmentation(file_path, max_time_steps=max_time_steps)\n",
    "\n",
    "        X.append(mel_spectrogram)\n",
    "        y.append(label)\n",
    "        y.append(label)\n",
    "        y.append(label)\n",
    "        y.append(label)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Save the preprocessed data\n",
    "    np.save('labels/X_train_MEL.npy', X)\n",
    "    np.save('labels/y_train_MEL.npy', y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the shapes for debugging\n",
    "print(\"Shape of X before reshape:\", X.shape)\n",
    "print(\"Shape of y before reshape:\", y.shape)\n",
    "\n",
    "# Reshape input data to match the required input shape for ResNet\n",
    "X_new = X.reshape((X.shape[0]*4, N_MELS, max_time_steps, 1))\n",
    "\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input shape and number of classes\n",
    "input_shape = X[0].shape\n",
    "num_classes = 2  # Assuming you have two classes (0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess evaluation data\n",
    "if os.path.exists('labels/val_X_MEL.npy') and os.path.exists('labels/val_y_MEL.npy'):\n",
    "    eval_X = np.load('labels/val_X_MEL.npy')\n",
    "    eval_y = np.load('labels/val_y_MEL.npy')\n",
    "else:\n",
    "    eval_X = []\n",
    "    eval_y = []\n",
    "\n",
    "    with open(VALIDATION_LABEL, 'r') as eval_label_file:\n",
    "        eval_lines = eval_label_file.readlines()\n",
    "\n",
    "    eval_labels = {}\n",
    "\n",
    "    for line in eval_lines:\n",
    "        parts = line.strip().split()\n",
    "        file_name = parts[1]\n",
    "        label = 1 if parts[-1] == \"bonafide\" else 0\n",
    "        eval_labels[file_name] = label\n",
    "\n",
    "    for file_name, label in eval_labels.items():\n",
    "        file_path = os.path.join(VALIDATION_DATA, file_name + \".flac\")\n",
    "\n",
    "        # Use the preprocess_data function\n",
    "        mel_spectrogram = preprocess_data_with_augmentation(file_path, max_time_steps=max_time_steps)\n",
    "\n",
    "        eval_X.append(mel_spectrogram)\n",
    "        eval_y.append(label)\n",
    "        eval_y.append(label)\n",
    "        eval_y.append(label)\n",
    "        eval_y.append(label)\n",
    "\n",
    "    eval_X = np.array(eval_X)\n",
    "    eval_y = np.array(eval_y)\n",
    "\n",
    "    # Save the preprocessed data\n",
    "    np.save('labels/val_X_MEL.npy', eval_X)\n",
    "    np.save('labels/val_y_MEL.npy', eval_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the shapes for debugging\n",
    "print(\"Shape of eval_X before reshape:\", eval_X.shape)\n",
    "print(\"Shape of eval_y before reshape:\", eval_y.shape)\n",
    "\n",
    "eval_X_reshaped = eval_X.reshape((eval_X.shape[0] * 4, N_MELS, max_time_steps, 1))\n",
    "\n",
    "print(eval_X_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def resnet_block(x, filters, kernel_size=3, stride=1, conv_shortcut=False):\n",
    "    shortcut = x\n",
    "    if conv_shortcut:\n",
    "        shortcut = layers.Conv2D(filters, (1, 1), strides=(stride, stride))(shortcut)\n",
    "        shortcut = layers.BatchNormalization()(shortcut)\n",
    "\n",
    "    x = layers.Conv2D(filters, (kernel_size, kernel_size), strides=(stride, stride), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv2D(filters, (kernel_size, kernel_size), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "# Build the ResNet model\n",
    "def build_resnet(input_shape, num_classes):\n",
    "    input_tensor = layers.Input(shape=input_shape)\n",
    "\n",
    "    x = layers.Conv2D(64, (7, 7), strides=(2, 2), padding='same')(input_tensor)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "    # ResNet blocks\n",
    "    for size in [64, 128, 256, 512]:\n",
    "        x = resnet_block(x, size, conv_shortcut=True)\n",
    "        x = resnet_block(x, size)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    x = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs=input_tensor, outputs=x, name='resnet_model')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=2\n",
    "\n",
    "# Assuming input_shape is defined as the shape of one sample in your data\n",
    "input_shape = (80, 109, 1)\n",
    "\n",
    "# Build the model\n",
    "model = build_resnet(input_shape=input_shape, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early Stopping\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = {i: w for i, w in zip(np.unique(y_train), class_weights)}\n",
    "\n",
    "print(class_weights_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with audio data augmentation\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "history = model.fit(X_train, y_train, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE,\n",
    "                            validation_data=(eval_X_reshaped, eval_y), class_weight = class_weights_dict, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Evaluate the model on the separate evaluation dataset\n",
    "eval_loss, eval_accuracy = model.evaluate(eval_X_reshaped, eval_y)\n",
    "print(f'Evaluation Loss: {eval_loss:.4f}, Evaluation Accuracy: {eval_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model.save(\"models/weighted_loss_augmentation.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
